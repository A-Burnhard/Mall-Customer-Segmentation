{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7acbmXNqF7UjOn+0YvQhc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A-Burnhard/Mall-Customer-Segmentation/blob/main/Mall_Customer_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG6RfAEw0Csy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading or importing  wine dataset to notebook\n",
        "cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
        "iris_data = pd.read_csv(\"iris.data\", names=cols)\n",
        "iris_data.head()"
      ],
      "metadata": {
        "id": "xf06jar70dSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**\n"
      ],
      "metadata": {
        "id": "pljBbpL10g5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Identifying Missing values**"
      ],
      "metadata": {
        "id": "bkNEmwVy0k-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify missing values\n",
        "missing_values = iris_data.isnull().sum()\n",
        "print(\"Missing values:\\n\", missing_values)"
      ],
      "metadata": {
        "id": "w5j2LkIr0fT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Identifying Duplicates**"
      ],
      "metadata": {
        "id": "hjGiFJ370q6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify duplicate rows\n",
        "duplicates = iris_data.duplicated()\n",
        "print(\"Duplicates instances: \\n\",duplicates)"
      ],
      "metadata": {
        "id": "fgzdSG8B0wdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outlier Detection using Boxplot**"
      ],
      "metadata": {
        "id": "IIYH2dRH03kL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "data = iris_data\n",
        "\n",
        "# Visualize the distribution of each feature using box plots\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=data)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Boxplot of Features')\n",
        "plt.show()\n",
        "\n",
        "# Identify outliers using statistical methods (e.g., Z-score or IQR)\n",
        "# Z-score method\n",
        "from scipy.stats import zscore\n",
        "\n",
        "data = iris_data.drop(\"class\", axis=1)\n",
        "z_scores = zscore(data)\n",
        "outlier_threshold = 3  # Adjust the threshold as per your preference\n",
        "outliers = (abs(z_scores) > outlier_threshold).any(axis=1)\n",
        "\n",
        "# IQR method\n",
        "Q1 = data.quantile(0.25)\n",
        "Q3 = data.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "outliers = ((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)\n",
        "\n",
        "# Count the number of outliers\n",
        "num_outliers = outliers.sum()\n",
        "print(f\"Number of outliers: {num_outliers}\")\n",
        "\n",
        "# Decide whether to remove outliers or transform them\n",
        "remove_outliers = False\n",
        "\n",
        "if remove_outliers:\n",
        "    # Remove outliers from the dataset\n",
        "    data = data[~outliers]\n",
        "    print(\"Outliers removed.\")\n",
        "else:\n",
        "    # Transform outliers to a specific value\n",
        "    outlier_value = 8  # Choose an appropriate value for transformation\n",
        "    data[outliers] = outlier_value\n",
        "    print(\"Outliers transformed.\")\n",
        "\n",
        "# Updated visualization after handling outliers\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=data)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Boxplot of Features (After Outlier Handling)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rzTWcbgC08dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining X and Y values**"
      ],
      "metadata": {
        "id": "Qc6B2KiI1DUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the target variable (class) from the features\n",
        "iris_data = pd.read_csv(\"iris.data\", names=cols)\n",
        "\n",
        "X = iris_data.drop('class', axis=1)\n",
        "y = iris_data['class']\n",
        "\n",
        "# Convert the target variable to numeric labels\n",
        "label_mapping = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n",
        "y = y.map(label_mapping)"
      ],
      "metadata": {
        "id": "NsRBcorI1JDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Influencial datapoint detection using leverage and cooks distance**"
      ],
      "metadata": {
        "id": "k7tEBzjs1c5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import OLSInfluence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Add a constant term to the features matrix for the intercept in the linear regression model\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit the linear regression model\n",
        "model = sm.OLS(y, X)\n",
        "results = model.fit()\n",
        "\n",
        "# Get the influence statistics\n",
        "influence = OLSInfluence(results)\n",
        "\n",
        "# Calculate the leverage values\n",
        "leverage = influence.hat_matrix_diag\n",
        "\n",
        "# Calculate the Cook's distance\n",
        "cooks_distance = influence.cooks_distance\n",
        "\n",
        "# Identify influential data points based on leverage or Cook's distance\n",
        "influential_points_leverage = leverage > 2 * (X.shape[1] + 1) / X.shape[0]\n",
        "influential_points_cooks = cooks_distance[0] > 4 / (X.shape[0] - X.shape[1] - 1)\n",
        "\n",
        "# Print the influential data points\n",
        "print(\"Influential points based on leverage:\")\n",
        "print(X[influential_points_leverage])\n",
        "\n",
        "print(\"\\nInfluential points based on Cook's distance:\")\n",
        "print(X[influential_points_cooks])\n"
      ],
      "metadata": {
        "id": "kKPcGq8S3oY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normality of the set of features using shapiro**"
      ],
      "metadata": {
        "id": "Yf2tWXuR3yxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import shapiro\n",
        "\n",
        "\n",
        "\n",
        "# Select the features to check for normality\n",
        "features = X\n",
        "\n",
        "# Perform Shapiro-Wilk test for each feature\n",
        "for column in features.columns:\n",
        "    stat, p_value = shapiro(features[column])\n",
        "    alpha = 0.05  # Significance level\n",
        "\n",
        "    print(f\"Feature: {column}\")\n",
        "    print(f\"Shapiro-Wilk test statistic: {stat}\")\n",
        "    print(f\"P-value: {p_value}\")\n",
        "\n",
        "    if p_value > alpha:\n",
        "        print(\"Feature appears to be normally distributed.\")\n",
        "    else:\n",
        "        print(\"Feature does not appear to be normally distributed.\")\n",
        "\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "AMMgQlSI4FUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data transformation**"
      ],
      "metadata": {
        "id": "l3nfDGc94K-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "X,y\n",
        "\n",
        "# Separate the target variable (class) from the features\n",
        "\n",
        "# Perform normalization using Min-Max scaler\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# Perform standardization using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_standardized = scaler.fit_transform(X)\n",
        "print(X)"
      ],
      "metadata": {
        "id": "LFX4wqu64RNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Selection**"
      ],
      "metadata": {
        "id": "OPvXspMD4XBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "# Create a Random Forest regressor\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "# Fit the Random Forest model\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get the feature importances\n",
        "feature_importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame of feature importances\n",
        "feature_importances_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
        "\n",
        "# Sort the features by importance (descending order)\n",
        "feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the feature importances\n",
        "print(feature_importances_df)"
      ],
      "metadata": {
        "id": "MBorDE7n4cFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Oversampling techniques using the Synthetic Minority Over-sampling Technique (SMOTE) to balance the imbalanced dataset**"
      ],
      "metadata": {
        "id": "EYsTmXBG4h_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "# Create a SMOTE object\n",
        "smote = SMOTE()\n",
        "\n",
        "# Perform oversampling\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Print the balanced class distribution\n",
        "print(\"Class distribution after SMOTE:\")\n",
        "print(y_resampled.value_counts())\n"
      ],
      "metadata": {
        "id": "JBhDLrah4oB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Selecting Appropriate Learners for Training and Validation (K means and agglomerative clustering)**"
      ],
      "metadata": {
        "id": "0BABWh4I4uwh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "27LFGwRw46xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K-Means clustering with K-fold cross-validation**"
      ],
      "metadata": {
        "id": "wOyyB-mn-cJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Instantiate the KMeans model\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "\n",
        "# Perform K-fold cross-validation\n",
        "k_fold_scores = cross_val_score(kmeans, X, cv=5)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"K-Fold Cross-Validation Scores for K-Means:\")\n",
        "print(k_fold_scores)\n",
        "\n"
      ],
      "metadata": {
        "id": "nsUhb4Pl-fye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hierachical clustering using agglomerative clusutering**"
      ],
      "metadata": {
        "id": "cKP9Y1yj-n83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Instantiate the AgglomerativeClustering model\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=3)\n",
        "\n",
        "# Perform leave-one-out cross-validation\n",
        "loo = LeaveOneOut()\n",
        "validation_scores = []\n",
        "\n",
        "for train_index, test_index in loo.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_pred = agg_clustering.fit_predict(X_train)\n",
        "    validation_scores.append(y_pred[test_index][0])\n",
        "\n",
        "# Print the validation scores\n",
        "print(\"Leave-One-Out Cross-Validation Scores for Hierarchical Clustering:\")\n",
        "print(validation_scores)\n"
      ],
      "metadata": {
        "id": "tk28vRRY_JBF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}